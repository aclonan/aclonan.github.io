---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, my name is Alex! I am currently a PhD Student at the University of Connecticut College of Engineering working with [Monty Escabi](https://www.bme.uconn.edu/faculty-staff/core-faculty/escabi-monty/) and [Ian Stevenson](https://psychology.uconn.edu/person/ian-stevenson/) in the [Physiological Acoustics Lab](https://escabilab.uconn.edu/). I graduated from the Univsersity of Connecticut in 2022, where I received my degrees in Electrical Engineering and Molecular Cell Biology.

My Current Research
======
**Low-dimensional interference of mid-level sound statistics predicts human speech recognition in natural environmental noise**
My current work is focused on the bottom-up acoustic cues that drive speech perception in natural enviornmental noise. Natural backgrounds can be quite diverse, with high degrees of spectrotemporal variability that arises from environmental acoustic generators. Whereas with speech, articulation imposes unique acoustic idiosyncrasies (i.e.: fundamental frequency, intonation) that influence our vocal quality, pronunciation, and phonetic implementation. What I am interested in, is how these acoustic cues are indicative of real-world human perception.

Papers
======
I am really excited to have my first publication in review, you can currently see it posted on [BioRxiv](https://www.biorxiv.org/content/10.1101/2024.02.13.579526v1). Here we assess the influence of bottom-up acoustic features of the foreground and background by adversarially positioning them against one another. This feature representation is inspired by the computations in the auditory midbrain. Our approach allows us to investigate perceptual transfer functions indicative of the cues we rely on for specific acoustic tasks. 

Machine and Human Perception
======
While my work is predominantly focused on human hearing, I am intersted in the differences between human and machine audition. Particularly, automated speech recognition systems, such as Alexa and Siri, often reach a point of failure in diverse enviornmental noise. In contrast, humans are innately able to disentangle the foreground target from complex auditory scenes. I am interested in why this dichotomy exists, how we can interrogate deep learning methods for ASR and how we can create effective intervention.

Sound Synthesis
======
I am also interested in applying machine learning methods, and optimization tools to create synthetic sounds to drive perceptual studies of audition. Preserving the natural diversity in auditory scenes, while addressing questions about niche acoustic features centric to how we perceive sounds (ie: Temporal Fine Structure, Spectral, Modulation Content, Reverberation).

Engineering Education
======
In tandem with my research, I am very interested in STEM Education, and diversity, equity and inclusion within the field. I work with the [Experiential Education Office at the Uconn College of Engineering](https://undergrad.engr.uconn.edu/experiential-education-staff/) to develop curriculum and mentor first-year engineering students. Working alongside [Nick Delaney](https://undergrad.engr.uconn.edu/advising-staff/nick-delaney-ece-advisor/), [Monica Bullock](https://undergrad.engr.uconn.edu/monica-bullock-program-administrator/) and [Jenn Pascal](https://chemical-biomolecular.engr.uconn.edu/people/faculty/pascal-jennifer/) we integrate service-learning initiatives into the curriculum to show students that their education can make an impact on their community.



