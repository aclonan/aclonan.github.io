---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, my name is Alex! I am currently a PhD Student at the University of Connecticut College of Engineering working with [Monty Escabi](https://www.bme.uconn.edu/faculty-staff/core-faculty/escabi-monty/) and [Ian Stevenson](https://psychology.uconn.edu/person/ian-stevenson/) in the [Physiological Acoustics Lab](https://escabilab.uconn.edu/). I am apart of the [Biomedical Engineering Program](https://www.bme.uconn.edu/academics/graduate-program/about-the-graduate-program-in-biomedical-engineering/), focusing my coursework on Machine Learning, Digital Signal Processing and Human Psychoacoustics. Further, I am apart of the [Institute for Brain and Cognitive Sciences](https://ibacs.uconn.edu/) where I partake in STEM Education Outreach around Connecticut. Bleeding blue, I also graduated from the Univsersity of Connecticut in 2022, where I received my undergraduate degrees in Electrical Engineering and Molecular Cell Biology. 

Reach me at @alexcarmenclo on X, or **alex**(dot)**clonan**(at)**uconn**(dot)**edu** via email. 

My Current Research
======
My current work is focused on the bottom-up acoustic cues that drive speech perception in natural enviornmental noise. Natural backgrounds can be quite diverse, with high degrees of spectrotemporal variability that arises from environmental acoustic generators. Whereas with speech, articulation imposes unique acoustic idiosyncrasies (i.e.: fundamental frequency, intonation) that influence our vocal quality, pronunciation, and phonetic implementation. What I am interested in, is how these acoustic cues interfere with one another, and are indicative of real-world human perception. See below more details about our paper on [BioRxiv!](https://www.biorxiv.org/content/10.1101/2024.02.13.579526v1)

Machine and Human Perception
======
While my work is predominantly focused on human hearing, I am intersted in the differences between human and machine audition. Particularly, automated speech recognition (ASR) systems, such as Alexa and Siri, often reach a point of failure in diverse enviornmental noise. In contrast, humans are innately able to disentangle the foreground target from complex auditory scenes. I am interested in why this dichotomy exists, how we can interrogate deep learning methods for ASR, to create biologically informed, and effective intervention.

Sound Synthesis
======
I am also interested in applying machine learning methods, and optimization tools to create synthetic sounds to drive perceptual studies of audition. Preserving the natural diversity in auditory scenes, while addressing questions about niche acoustic features centric to how we perceive sounds (ie: Temporal Fine Structure, Spectral, Modulation Content, Reverberation). 

Engineering Education
======
In tandem with my research, I am very interested in STEM Education, and diversity, equity and inclusion initiatives within the field. I work with the [Experiential Education Office at the Uconn College of Engineering](https://undergrad.engr.uconn.edu/experiential-education-staff/) to develop curriculum and mentor first-year engineering students. Working alongside [Nick Delaney](https://undergrad.engr.uconn.edu/advising-staff/nick-delaney-ece-advisor/), [Monica Bullock](https://undergrad.engr.uconn.edu/monica-bullock-program-administrator/) and [Jenn Pascal](https://chemical-biomolecular.engr.uconn.edu/people/faculty/pascal-jennifer/) we integrate service-learning initiatives into the curriculum to show students that their education can make an impact on their community.

Publications
======
### **Low-dimensional interference of mid-level sound statistics predicts human speech recognition in natural environmental noise (in Review, 2024)**
### Alex C. Clonan, Xiu Zhai, Ian H. Stevenson, Monty A. Escab√≠
I am really excited to our recent work in review, you can currently see it posted on [BioRxiv](https://www.biorxiv.org/content/10.1101/2024.02.13.579526v1). Here we assess the influence of bottom-up acoustic features of the foreground and background by adversarially positioning them against one another. This feature representation is inspired by the computations in the auditory midbrain (IC). Our approach allows us to investigate perceptual transfer functions indicative of the cues we rely on for specific acoustic tasks. 




